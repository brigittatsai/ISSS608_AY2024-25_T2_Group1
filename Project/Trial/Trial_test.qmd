---
title: "Group_Project"
author: "Hannah"
date: "March 19, 2025"
date-modified: "last-modified"
execute: 
  eval: true
  echo: true
  message: false
  freeze: true
---

## 1 Weather Data (Download and Merged)

```{r}
pacman::p_load(rvest,dplyr,stringr,purrr,readr,httr,tidyr,fs,janitor,tidyverse,knitr)
#| eval: false
#| include: false
#This is the code where I downloaded the data：
# The base URL of the target website
base_url <- "https://www.weather.gov.sg/files/dailydata/"

# Site corresponding file code
stations <- c("Changi" = "S24", "Ang_Mo_Kio" = "S109", "Pulau_Ubin" = "S106", 
              "East_Coast_Parkway" = "S107", "Seletar" = "S25",
              "Clementi" = "S50", "Jurong_West" = "S44", "Paya_Lebar" = "S06",
              "Newton" = "S111", "Pasir_Panjang" = "S116", "Tai_Seng" = "S43",
              "Admiralty" = "S104", "Sembawang" = "S80", "Sentosa_Island" = "S60",
              "Jurong_Island" = "S117")

# Define download time range (January 2019 - January 2025)
years <- 2024:2025
months <- sprintf("%02d", 1:12)  # 01, 02, ..., 12

# Only data from January 2019 to January 2025 are retained
date_combinations <- expand.grid(Year = years, Month = months, stringsAsFactors = FALSE) %>%
  filter(!(Year == 2024 & Month < "01"), # Exclude before January 2019
         !(Year == 2025 & Month > "01")) # Exclude after January 2025

# Create a directory to store data
dir.create("data", showWarnings = FALSE)

# Record failed downloads
failed_downloads <- data.frame(Station = character(), Year = integer(), Month = character(), File_URL = character(), stringsAsFactors = FALSE)

# Iterate through stations, years, and months
for (station_name in names(stations)) {
  station_code <- stations[[station_name]]
  
  for (i in 1:nrow(date_combinations)) {
    year <- date_combinations$Year[i]
    month <- date_combinations$Month[i]
    
    # Construct the file name
    file_name <- paste0("DAILYDATA_", station_code, "_", year, month, ".csv")
    
    # Construct the full download URL
    file_url <- paste0(base_url, file_name)

    # Local save path
    file_path <- file.path("data", file_name)
    
    # Check if the URL is valid
    response <- HEAD(file_url)
    
    if (status_code(response) == 200) {
      # Download the file
      download.file(file_url, destfile = file_path, mode = "wb")
      cat("✅ Download successful:", file_name, "\n")
    } else {
      cat("❌ Download failed:", file_name, "\n")
      
      # Record the failed file
      failed_downloads <- rbind(failed_downloads, data.frame(
        Station = station_name,
        Year = year,
        Month = month,
        File_URL = file_url
      ))
    }
  }
}

# Save the failed download log
if (nrow(failed_downloads) > 0) {
  write.csv(failed_downloads, "data/failed_downloads_log.csv", row.names = FALSE)
  cat("⚠️ The failed download files have been recorded: data/failed_downloads_log.csv\n")
} else {
  cat("🎉 All data downloaded successfully!\n")
}
```

```{r}
library(tidyverse)
library(janitor)

# 取得所有 2024 年的 CSV 檔案
files <- list.files("data", pattern = "^DAILYDATA_.*_2024.*\\.csv$", full.names = TRUE)

# 讀取函式，確保所有欄位初始為 character
safe_read <- safely(~ read_csv(
  .x,
  col_types = cols(.default = "c"),  # 先讀成文字，避免合併錯誤
  locale = locale(encoding = "UTF-8")
) %>%
  clean_names())  # 統一欄位名稱

# 讀取所有檔案
results <- map(files, safe_read)

# 過濾成功的資料並合併
all_data <- results %>%
  keep(~ is.null(.x$error)) %>%
  map_dfr("result") %>%
  mutate(across(where(is.character) & !matches("station"), ~ na_if(., "-"))) %>%  # "-" 轉 NA（但 Station 保留）
  mutate(across(where(is.character) & !matches("station"), as.numeric))  # 轉回數字（Station 不變）

# 確認合併結果
glimpse(all_data)

```

```{r}
pacman::p_load(sp, sf, raster, spatstat, tmap, tidyverse,
               spNetwork, tmaptools)
```

