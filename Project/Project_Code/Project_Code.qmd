---
title: "Project Code"
author: "Brigitta Karen Tsai, Enrico Limberg, TzuTing Huang"
date: "March 19, 2025"
date-modified: "last-modified"
execute: 
  eval: true
  echo: true
  message: false
  freeze: true
---

# 1 Data Extraction

## 1.1 Install and Load R Packages

```{r}
pacman::p_load(rvest,dplyr,stringr,purrr,
               readr,httr,tidyr,fs,janitor,
               tidyverse,knitr)
```

## 1.2 Download data

```{r, eval=FALSE}
#This is the code where I downloaded the dataÔºö
base_url <- "https://www.weather.gov.sg/files/dailydata/"

# Site corresponding file code
stations <- c("Changi"="S24", "Ang Mo Kio"="S109", "Pulau Ubin"="S106", 
              "East Coast Parkway"="S107", "Seletar"="S25",
              "Clementi"="S50", "Jurong (West)"="S44", "Paya Lebar"="S06",
              "Newton"="S111", "Pasir Panjang"="S116", "Tai Seng"="S43",
              "Admiralty" = "S104", "Sembawang" = "S80", "Sentosa Island" = "S60",
              "Jurong Island" = "S117", "Choa Chu Kang (South)"="S121",
              "Tuas South"="S115")

# Define download time range
years <- 2024:2024
months <- sprintf("%02d", 1:12)  # 01, 02, ..., 12

# Only data from January 2024 to December 2024 are retained
date_combinations <- expand.grid(Year = years, Month = months, stringsAsFactors = FALSE) %>%
  filter(!(Year == 2024 & Month < "01"), # Exclude before Jan 2024
         !(Year == 2024 & Month > "12")) # Exclude after Dec 2024

# Create a directory to store data
dir.create("data", showWarnings = FALSE)

# Record failed downloads
failed_downloads <- data.frame(Station = character(), Year = integer(), Month = character(), File_URL = character(), stringsAsFactors = FALSE)

# Iterate through stations, years, and months
for (station_name in names(stations)) {
  station_code <- stations[[station_name]]
  
  for (i in 1:nrow(date_combinations)) {
    year <- date_combinations$Year[i]
    month <- date_combinations$Month[i]
    
    # Construct the file name
    file_name <- paste0("DAILYDATA_", station_code, "_", year, month, ".csv")
    
    # Construct the full download URL
    file_url <- paste0(base_url, file_name)

    # Local save path
    file_path <- file.path("data", file_name)
    
    # Check if the URL is valid
    response <- HEAD(file_url)
    
    if (status_code(response) == 200) {
      # Download the file
      download.file(file_url, destfile = file_path, mode = "wb")
      cat("‚úÖ Download successful:", file_name, "\n")
    } else {
      cat("‚ùå Download failed:", file_name, "\n")
      
      # Record the failed file
      failed_downloads <- rbind(failed_downloads, data.frame(
        Station = station_name,
        Year = year,
        Month = month,
        File_URL = file_url
      ))
    }
  }
}

# Save the failed download log
if (nrow(failed_downloads) > 0) {
  write.csv(failed_downloads, "data/failed_downloads_log.csv", row.names = FALSE)
  cat("‚ö†Ô∏è The failed download files have been recorded: data/failed_downloads_log.csv\n")
} else {
  cat("üéâ All data downloaded successfully!\n")
}
```

## 1.3 Merge data

```{r}
library(tidyverse)
library(janitor)

# Get all CSV files from the "data" folder that contain "DAILYDATA_" and "2024" in their names
files <- list.files("data", pattern = "^DAILYDATA_.*_2024.*\\.csv$", full.names = TRUE)

# Define a safe reading function to ensure all columns are initially read as characters
safe_read <- safely(~ read_csv(
  .x,
  col_types = cols(.default = "c"),  # Read all columns as character to prevent merging errors
  locale = locale(encoding = "UTF-8")  # Ensure UTF-8 encoding
) %>%
  clean_names())  # Standardize column names

# Read all files
results <- map(files, safe_read)

# Filter out successful reads and combine data
all_data <- results %>%
  keep(~ is.null(.x$error)) %>%  # Keep only successfully read files
  map_dfr("result") %>%  # Combine them into a single dataframe
  mutate(across(where(is.character) & !matches("station"), ~ na_if(., "-"))) %>%  # Convert "-" to NA (except for "station" column)
  mutate(across(where(is.character) & !matches("station"), as.numeric))  # Convert character columns back to numeric (except "station")

# Check the merged data structure
glimpse(all_data)


```

## 1.4 Save to csv File

```{r}
write_csv(all_data, "data/weather.csv", na = "", col_names = TRUE)
```

## 1.5 Remove Individual Files

```{r}
# Delete all CSV files in the "data" folder
file.remove(list.files("data", pattern = "^DAILYDATA_.*_2024.*\\.csv$", full.names = TRUE))

cat("üóëÔ∏è All individual CSV files have been deleted.\n")
```

# 2 Data Preparation

```{r}
pacman::p_load(sp, sf, raster, spatstat, tmap, tidyverse,
               spNetwork, tmaptools)
```
